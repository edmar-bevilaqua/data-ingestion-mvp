{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c629feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "\n",
    "# (recomendado) JDK 17\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-20\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Hadoop winutils (necessário no Windows)\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PATH\"] = os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Use o Python do próprio kernel/notebook para driver e executors\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "\n",
    "# Diretório temporário estável (evita bloqueio do antivírus)\n",
    "pathlib.Path(r\"C:\\spark-tmp\").mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = r\"C:\\spark-tmp\"\n",
    "os.environ[\"TMP\"] = r\"C:\\spark-tmp\"\n",
    "os.environ[\"TEMP\"] = r\"C:\\spark-tmp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a829ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark com Delta Lake ✅\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Edmar:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>cnpj-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2549aa1ded0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def start_spark_in_notebook(app_name=\"cnpj-notebook\"):\n",
    "    base = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .config(\"spark.sql.session.timeZone\",\"UTC\")\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "        .config(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "        .config(\"spark.sql.shuffle.partitions\",\"4\")\n",
    "        # garante o Python deste kernel\n",
    "        .config(\"spark.pyspark.python\", sys.executable)\n",
    "        .config(\"spark.pyspark.driver.python\", sys.executable)\n",
    "        # estabilidade/diagnóstico no Windows\n",
    "        .config(\"spark.python.worker.reuse\",\"false\")\n",
    "        .config(\"spark.python.worker.faulthandler.enabled\",\"true\")\n",
    "        .config(\"spark.sql.execution.pyspark.udf.faulthandler.enabled\",\"true\")\n",
    "        .config(\"spark.local.dir\", os.environ.get(\"SPARK_LOCAL_DIRS\", r\"C:\\spark-tmp\"))\n",
    "        .config(\"spark.network.timeout\",\"300s\")\n",
    "        .config(\"spark.executor.heartbeatInterval\",\"60s\")\n",
    "    )\n",
    "    try:\n",
    "        # se delta-spark estiver instalado, habilita Delta Lake\n",
    "        from delta import configure_spark_with_delta_pip\n",
    "        spark = configure_spark_with_delta_pip(base\n",
    "            .config(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "        ).getOrCreate()\n",
    "        print(\"Spark com Delta Lake ✅\")\n",
    "        return spark\n",
    "    except Exception:\n",
    "        spark = base.getOrCreate()\n",
    "        print(\"Spark sem Delta (Parquet) ✅\")\n",
    "        return spark\n",
    "\n",
    "spark = start_spark_in_notebook()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4bef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, io, zipfile, hashlib, shutil\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "BASE_URL = \"https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/\"\n",
    "RAW_DIR  = Path(\"./data/_raw\").resolve()\n",
    "OUT_DIR  = Path(\"./data/dev_out\").resolve()         # pasta “dev” pra não misturar com produção\n",
    "LOG_DIR  = Path(\"./data/_metadata\").resolve()\n",
    "LOG_PATH = LOG_DIR / \"ingestion_log_spark.parquet\"  # log só do Spark\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def month_dirs() -> List[str]:\n",
    "    \"\"\"Lista subdiretórios AAAA-MM/ disponíveis na raiz.\"\"\"\n",
    "    html = requests.get(BASE_URL, timeout=60).text\n",
    "    # captura AAAA-MM/ (com barra ao final)\n",
    "    return sorted(set(re.findall(r'href=\"(\\d{4}-\\d{2}/)\"', html)))\n",
    "\n",
    "def list_zip_urls(month:str) -> List[Tuple[str,str]]:\n",
    "    \"\"\"Lista (nome, url) dos .zip do mês informado (ex: '2025-09/').\"\"\"\n",
    "    url = BASE_URL + month\n",
    "    html = requests.get(url, timeout=60).text\n",
    "    zips = re.findall(r'href=\"([^\"]+\\.zip)\"', html, flags=re.IGNORECASE)\n",
    "    return [(z, url + z) for z in zips]\n",
    "\n",
    "def download(url:str, dest:Path) -> Dict[str,str]:\n",
    "    \"\"\"Baixa arquivo c/ HEAD+GET e retorna metadados simples.\"\"\"\n",
    "    with requests.Session() as s:\n",
    "        h = s.head(url, timeout=60)\n",
    "        h.raise_for_status()\n",
    "        etag = h.headers.get(\"ETag\",\"\")\n",
    "        lm   = h.headers.get(\"Last-Modified\",\"\")\n",
    "        cl   = h.headers.get(\"Content-Length\",\"\")\n",
    "        r = s.get(url, timeout=600, stream=True)\n",
    "        r.raise_for_status()\n",
    "        m = hashlib.md5()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1024*1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk); m.update(chunk)\n",
    "    return {\n",
    "        \"etag\": etag, \"last_modified\": lm, \"content_length\": cl,\n",
    "        \"content_md5\": m.hexdigest()\n",
    "    }\n",
    "\n",
    "def sniff_dataset_from_zipname(zip_name:str) -> str:\n",
    "    \"\"\"Mapeia o zip para um nome de dataset amigável (heurística).\"\"\"\n",
    "    n = zip_name.lower()\n",
    "    if \"empresa\" in n: return \"empresas\"\n",
    "    if \"estabele\" in n: return \"estabelecimentos\"\n",
    "    if \"simples\" in n: return \"dados_do_simples\"\n",
    "    if \"socio\" in n: return \"socios\"\n",
    "    if \"pais\" in n: return \"paises\"\n",
    "    if \"municip\" in n: return \"municipios\"\n",
    "    if \"qualific\" in n: return \"qualificacoes_socios\"\n",
    "    if \"natureza\" in n: return \"naturezas_juridicas\"\n",
    "    if \"cnae\" in n: return \"cnaes\"\n",
    "    return \"desconhecido\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d5b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "SCHEMAS = {\n",
    "    \"empresas\": {\n",
    "        \"columns\": [\n",
    "            (\"cnpj_basico\",\"string\"),\n",
    "            (\"razao_social\",\"string\"),\n",
    "            (\"natureza_juridica\",\"string\"),\n",
    "            (\"qualificacao_responsavel\",\"string\"),\n",
    "            (\"capital_social\",\"double\"),\n",
    "            (\"porte\",\"string\"),\n",
    "            (\"ente_federativo_responsavel\",\"string\"),\n",
    "        ]\n",
    "    },\n",
    "    \"estabelecimentos\": {\n",
    "        \"columns\": [\n",
    "            (\"cnpj_basico\",\"string\"),(\"cnpj_ordem\",\"string\"),(\"cnpj_dv\",\"string\"),\n",
    "            (\"identificador_matriz_filial\",\"byte\"),(\"nome_fantasia\",\"string\"),\n",
    "            (\"situacao_cadastral\",\"string\"),(\"data_situacao_cadastral\",\"string\"),\n",
    "            (\"motivo_situacao_cadastral\",\"string\"),(\"nome_cidade_exterior\",\"string\"),\n",
    "            (\"pais\",\"string\"),(\"data_inicio_atividade\",\"string\"),\n",
    "            (\"cnae_fiscal_principal\",\"string\"),(\"cnae_fiscal_secundaria\",\"string\"),\n",
    "            (\"tipo_logradouro\",\"string\"),(\"logradouro\",\"string\"),(\"numero\",\"string\"),\n",
    "            (\"complemento\",\"string\"),(\"bairro\",\"string\"),(\"cep\",\"string\"),(\"uf\",\"string\"),\n",
    "            (\"municipio\",\"string\"),(\"ddd_telefone_1\",\"string\"),(\"telefone_1\",\"string\"),\n",
    "            (\"ddd_telefone_2\",\"string\"),(\"telefone_2\",\"string\"),(\"ddd_fax\",\"string\"),\n",
    "            (\"fax\",\"string\"),(\"email\",\"string\"),(\"situacao_especial\",\"string\"),\n",
    "            (\"data_situacao_especial\",\"string\"),\n",
    "        ]\n",
    "    },\n",
    "    \"dados_do_simples\": {\n",
    "        \"columns\": [\n",
    "            (\"cnpj_basico\",\"string\"),(\"opcao_pelo_simples\",\"string\"),\n",
    "            (\"data_opcao_pelo_simples\",\"string\"),(\"data_exclusao_do_simples\",\"string\"),\n",
    "            (\"opcao_pelo_mei\",\"string\"),(\"data_opcao_pelo_mei\",\"string\"),\n",
    "            (\"data_exclusao_do_mei\",\"string\"),\n",
    "        ]\n",
    "    },\n",
    "    \"socios\": {\n",
    "        \"columns\": [\n",
    "            (\"cnpj_basico\",\"string\"),(\"identificador_socio\",\"string\"),\n",
    "            (\"nome_socio\",\"string\"),(\"cnpj_cpf_socio\",\"string\"),\n",
    "            (\"qualificacao_socio\",\"string\"),(\"data_entrada_sociedade\",\"string\"),\n",
    "            (\"pais\",\"string\"),(\"representante_legal\",\"string\"),\n",
    "            (\"nome_representante_legal\",\"string\"),\n",
    "            (\"qualificacao_representante_legal\",\"string\"),\n",
    "            (\"faixa_etaria\",\"string\"),\n",
    "        ]\n",
    "    },\n",
    "    \"paises\": {\"columns\":[(\"codigo\",\"string\"),(\"descricao\",\"string\")]},\n",
    "    \"municipios\": {\"columns\":[(\"codigo\",\"string\"),(\"descricao\",\"string\")]},\n",
    "    \"qualificacoes_socios\": {\"columns\":[(\"codigo\",\"string\"),(\"descricao\",\"string\")]},\n",
    "    \"naturezas_juridicas\": {\"columns\":[(\"codigo\",\"string\"),(\"descricao\",\"string\")]},\n",
    "    \"cnaes\": {\"columns\":[(\"codigo\",\"string\"),(\"descricao\",\"string\")]},\n",
    "}\n",
    "\n",
    "def pick_schema_for_zip(zip_name:str) -> Tuple[str, List[Tuple[str,str]]]:\n",
    "    # 1) tentativa direta pelo zip\n",
    "    ds = sniff_dataset_from_zipname(zip_name)\n",
    "    if ds in SCHEMAS:\n",
    "        return ds, SCHEMAS[ds][\"columns\"]\n",
    "    # 2) aproximação por nome\n",
    "    best = get_close_matches(ds, list(SCHEMAS.keys()), n=1, cutoff=0.6)\n",
    "    if best:\n",
    "        return best[0], SCHEMAS[best[0]][\"columns\"]\n",
    "    # fallback\n",
    "    return \"desconhecido\", []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b7e1b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-09/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Descobrir meses e escolher o mais recente\n",
    "months = month_dirs()\n",
    "months[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4478f2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Cnaes.zip',\n",
       " 'https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/2025-09/Cnaes.zip')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Listar .zip do mês alvo\n",
    "target_month = months[-1]      # ex.: '2025-09/'\n",
    "zips = list_zip_urls(target_month)\n",
    "zips[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff12fd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Cnaes.zip',\n",
       " 'https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/2025-09/Cnaes.zip')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Escolher um ZIP “leve” para teste — ex.: Cnaes.zip\n",
    "chosen = None\n",
    "for name, url in zips:\n",
    "    if \"cnae\" in name.lower():\n",
    "        chosen = (name, url)\n",
    "        break\n",
    "chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e56381cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('C:/Users/edmar/MeusProjetos/white-cube/data-ingestion-mvp/data/_raw/2025-09_Cnaes.zip'),\n",
       " {'etag': '\"563e-63ec7eb241b5c\"',\n",
       "  'last_modified': 'Sun, 14 Sep 2025 19:30:24 GMT',\n",
       "  'content_length': '22078',\n",
       "  'content_md5': '48c53ff64378f2546af1546b4024ac3e'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Baixar o ZIP e inspecionar conteúdo\n",
    "zip_name, zip_url = chosen\n",
    "local_zip = RAW_DIR / f\"{target_month.strip('/')}_{zip_name}\"\n",
    "meta = download(zip_url, local_zip)\n",
    "local_zip, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9a36b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F.K03200$Z.D50913.CNAECSV']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Ver arquivos internos do ZIP\n",
    "with zipfile.ZipFile(local_zip, \"r\") as z:\n",
    "    members = z.namelist()\n",
    "members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "587f8c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F.K03200$Z.D50913.CNAECSV.csv']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TMP_EXTRACT = RAW_DIR / \"_tmp_extract\"\n",
    "if TMP_EXTRACT.exists():\n",
    "    shutil.rmtree(TMP_EXTRACT)\n",
    "TMP_EXTRACT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(local_zip, \"r\") as z:\n",
    "    for m in z.infolist():\n",
    "        if m.is_dir():\n",
    "            continue\n",
    "        # nomes \"estranhos\" -> força .csv\n",
    "        out_name = Path(m.filename).name\n",
    "        if not out_name.lower().endswith(\".csv\"):\n",
    "            out_name = out_name + \".csv\"\n",
    "        z.extract(m, TMP_EXTRACT)\n",
    "        src = TMP_EXTRACT / m.filename\n",
    "        dst = TMP_EXTRACT / out_name\n",
    "        if src != dst:\n",
    "            src.replace(dst)\n",
    "\n",
    "sorted([p.name for p in TMP_EXTRACT.iterdir() if p.is_file()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbffbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/edmar/MeusProjetos/white-cube/data-ingestion-mvp/data/_raw/_tmp_extract/F.K03200$Z.D50913.CNAECSV.csv')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pega o primeiro CSV (no dataset CNAE deve existir só 1)\n",
    "csv_path = sorted(TMP_EXTRACT.glob(\"*.csv\"))[0]\n",
    "csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db2f309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------------+\n",
      "|codigo |descricao                                                |\n",
      "+-------+---------------------------------------------------------+\n",
      "|0111301|Cultivo de arroz                                         |\n",
      "|0111302|Cultivo de milho                                         |\n",
      "|0111303|Cultivo de trigo                                         |\n",
      "|0111399|Cultivo de outros cereais não especificados anteriormente|\n",
      "|0112101|Cultivo de algodão herbáceo                              |\n",
      "+-------+---------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- codigo: string (nullable = true)\n",
      " |-- descricao: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema Spark a partir do dicionário (sem cabeçalho no arquivo)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ByteType, DoubleType\n",
    "\n",
    "ds_name, schema_cols = pick_schema_for_zip(zip_name)\n",
    "assert ds_name != \"desconhecido\", f\"Schema não identificado para {zip_name}\"\n",
    "\n",
    "def to_spark_type(t:str):\n",
    "    t = t.lower()\n",
    "    if t in (\"string\",\"str\"): return StringType()\n",
    "    if t in (\"byte\",\"int8\"): return ByteType()\n",
    "    if t in (\"double\",\"float64\",\"float\",\"decimal\"): return DoubleType()\n",
    "    # fallback\n",
    "    return StringType()\n",
    "\n",
    "schema = StructType([StructField(col, to_spark_type(tp), True) for col,tp in schema_cols])\n",
    "\n",
    "# Lê CSV sem header, separador ; (o padrão desses arquivos), com encoding latin-1\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\",\"false\")\n",
    "      .option(\"sep\",\";\")\n",
    "      .option(\"encoding\",\"ISO-8859-1\")\n",
    "      .schema(schema)\n",
    "      .load(str(csv_path)))\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dba02741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gravado em: C:\\Users\\edmar\\MeusProjetos\\white-cube\\data-ingestion-mvp\\data\\dev_out\\cnaes | delta? False\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "ref_month = target_month.strip(\"/\")   # ex: '2025-09'\n",
    "df2 = df.withColumn(\"ref_month\", F.lit(ref_month))\n",
    "\n",
    "# Ajuste de número de arquivos ao escrever (para o teste, 1 arquivo):\n",
    "df2 = df2.coalesce(1)\n",
    "\n",
    "# Escolhe formato conforme sua sessão:\n",
    "USE_DELTA = False\n",
    "\n",
    "\n",
    "dest = OUT_DIR / ds_name\n",
    "if USE_DELTA:\n",
    "    (df2.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"ref_month\")\n",
    "        .save(str(dest)))\n",
    "else:\n",
    "    (df2.write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"ref_month\")\n",
    "        .parquet(str(dest)))\n",
    "\n",
    "print(\"gravado em:\", dest, \"| delta?\" , USE_DELTA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4119c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------------+---------+\n",
      "|codigo |descricao                                                |ref_month|\n",
      "+-------+---------------------------------------------------------+---------+\n",
      "|0111301|Cultivo de arroz                                         |2025-09  |\n",
      "|0111302|Cultivo de milho                                         |2025-09  |\n",
      "|0111303|Cultivo de trigo                                         |2025-09  |\n",
      "|0111399|Cultivo de outros cereais não especificados anteriormente|2025-09  |\n",
      "|0112101|Cultivo de algodão herbáceo                              |2025-09  |\n",
      "+-------+---------------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "+---------+\n",
      "|ref_month|\n",
      "+---------+\n",
      "|  2025-09|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if USE_DELTA:\n",
    "    outdf = spark.read.format(\"delta\").load(str(dest))\n",
    "else:\n",
    "    outdf = spark.read.parquet(str(dest))\n",
    "\n",
    "outdf.where(F.col(\"ref_month\")==ref_month).show(5, truncate=False)\n",
    "outdf.select(\"ref_month\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "606b4fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------+---------+-------+---------+--------------------+-----------------------------+--------------+--------------------------------+--------------------------+\n",
      "|file_url                                                                              |zip_name |dataset|ref_month|etag                |last_modified                |content_length|content_md5                     |processed_at              |\n",
      "+--------------------------------------------------------------------------------------+---------+-------+---------+--------------------+-----------------------------+--------------+--------------------------------+--------------------------+\n",
      "|https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/2025-09/Cnaes.zip|Cnaes.zip|cnaes  |2025-09  |\"563e-63ec7eb241b5c\"|Sun, 14 Sep 2025 19:30:24 GMT|22078         |48c53ff64378f2546af1546b4024ac3e|2025-09-18 20:06:23.267002|\n",
      "+--------------------------------------------------------------------------------------+---------+-------+---------+--------------------+-----------------------------+--------------+--------------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType\n",
    "\n",
    "log_schema = (StructType()\n",
    "    .add(\"file_url\",\"string\")\n",
    "    .add(\"zip_name\",\"string\")\n",
    "    .add(\"dataset\",\"string\")\n",
    "    .add(\"ref_month\",\"string\")\n",
    "    .add(\"etag\",\"string\")\n",
    "    .add(\"last_modified\",\"string\")\n",
    "    .add(\"content_length\",\"string\")\n",
    "    .add(\"content_md5\",\"string\")\n",
    "    .add(\"processed_at\", TimestampType()))\n",
    "\n",
    "def read_log() -> 'DataFrame':\n",
    "    p = LOG_PATH\n",
    "    if p.exists():\n",
    "        return spark.read.parquet(str(p))\n",
    "    else:\n",
    "        return spark.createDataFrame([], log_schema)\n",
    "\n",
    "def write_log_append(rows_df):\n",
    "    (rows_df\n",
    "     .coalesce(1)\n",
    "     .write\n",
    "     .mode(\"append\")\n",
    "     .parquet(str(LOG_PATH)))\n",
    "\n",
    "logdf = read_log()\n",
    "logdf.orderBy(\"processed_at\", ascending=False).show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b144759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checagem de já processado (por hash ou ETag+len+lm+ref_month)\n",
    "def already_processed(logdf, file_url, ref_month, content_md5, etag, last_modified, content_length) -> bool:\n",
    "    cond = (\n",
    "        (logdf.file_url == file_url) &\n",
    "        (logdf.ref_month == ref_month) &\n",
    "        ((logdf.content_md5 == content_md5) | (\n",
    "            (logdf.etag == etag) & (logdf.last_modified == last_modified) & (logdf.content_length == content_length)\n",
    "        ))\n",
    "    )\n",
    "    return logdf.filter(cond).limit(1).count() > 0\n",
    "\n",
    "ap = already_processed(logdf, zip_url, ref_month, meta[\"content_md5\"], meta[\"etag\"], meta[\"last_modified\"], meta[\"content_length\"])\n",
    "ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a810d42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log atualizado\n"
     ]
    }
   ],
   "source": [
    "# Se não processado, append no log com processed_at=now()\n",
    "if not ap:\n",
    "    now = datetime.utcnow()\n",
    "    rows = [(zip_url, zip_name, ds_name, ref_month,\n",
    "             meta[\"etag\"], meta[\"last_modified\"], meta[\"content_length\"], meta[\"content_md5\"], now)]\n",
    "    to_append = spark.createDataFrame(rows, schema=log_schema)\n",
    "    write_log_append(to_append)\n",
    "    print(\"Log atualizado\")\n",
    "else:\n",
    "    print(\"Já processado anteriormente; log mantido\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a22d47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+\n",
      "|dataset|ref_month|count|\n",
      "+-------+---------+-----+\n",
      "|cnaes  |2025-09  |1    |\n",
      "+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdf = read_log()\n",
    "logdf.groupBy(\"dataset\",\"ref_month\").count().orderBy(\"dataset\",\"ref_month\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f462cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️  Mês alvo: 2025-09\n",
      "🧾 37 arquivos .zip encontrados no mês 2025-09.\n",
      "💾 Formato de saída: Parquet\n",
      "\n",
      "— Baixando: Cnaes.zip  → dataset='cnaes'\n",
      "  ✓ já processado anteriormente (mesmos metadados) — pulando.\n",
      "\n",
      "— Baixando: Empresas0.zip  → dataset='empresas'\n",
      "  ✅ ok: Empresas0.zip → empresas (rows=24038054)\n",
      "\n",
      "— Baixando: Empresas1.zip  → dataset='empresas'\n",
      "  ✅ ok: Empresas1.zip → empresas (rows=4494860)\n",
      "\n",
      "— Baixando: Empresas2.zip  → dataset='empresas'\n",
      "  ✅ ok: Empresas2.zip → empresas (rows=4494860)\n",
      "\n",
      "— Baixando: Empresas3.zip  → dataset='empresas'\n",
      "  ✅ ok: Empresas3.zip → empresas (rows=4494860)\n",
      "\n",
      "— Baixando: Empresas4.zip  → dataset='empresas'\n",
      "  ✅ ok: Empresas4.zip → empresas (rows=4494860)\n",
      "\n",
      "— Baixando: Empresas5.zip  → dataset='empresas'\n",
      "  ✅ ok: Empresas5.zip → empresas (rows=4494860)\n",
      "\n",
      "— Baixando: Empresas6.zip  → dataset='empresas'\n",
      "  ✅ ok: Empresas6.zip → empresas (rows=4494860)\n",
      "\n",
      "— Baixando: Empresas7.zip  → dataset='empresas'\n",
      "  ✅ ok: Empresas7.zip → empresas (rows=4494860)\n",
      "\n",
      "— Baixando: Empresas8.zip  → dataset='empresas'\n",
      "  ✅ ok: Empresas8.zip → empresas (rows=4494860)\n",
      "\n",
      "— Baixando: Empresas9.zip  → dataset='empresas'\n",
      "  ✅ ok: Empresas9.zip → empresas (rows=4494860)\n",
      "\n",
      "— Baixando: Estabelecimentos0.zip  → dataset='estabelecimentos'\n",
      "  ✅ ok: Estabelecimentos0.zip → estabelecimentos (rows=24859849)\n",
      "\n",
      "— Baixando: Estabelecimentos1.zip  → dataset='estabelecimentos'\n",
      "  ✅ ok: Estabelecimentos1.zip → estabelecimentos (rows=4753435)\n",
      "\n",
      "— Baixando: Estabelecimentos2.zip  → dataset='estabelecimentos'\n",
      "  ✅ ok: Estabelecimentos2.zip → estabelecimentos (rows=4753435)\n",
      "\n",
      "— Baixando: Estabelecimentos3.zip  → dataset='estabelecimentos'\n",
      "  ✅ ok: Estabelecimentos3.zip → estabelecimentos (rows=4753435)\n",
      "\n",
      "— Baixando: Estabelecimentos4.zip  → dataset='estabelecimentos'\n",
      "  ✅ ok: Estabelecimentos4.zip → estabelecimentos (rows=4753435)\n",
      "\n",
      "— Baixando: Estabelecimentos5.zip  → dataset='estabelecimentos'\n",
      "  ✅ ok: Estabelecimentos5.zip → estabelecimentos (rows=4753437)\n",
      "\n",
      "— Baixando: Estabelecimentos6.zip  → dataset='estabelecimentos'\n",
      "  ✅ ok: Estabelecimentos6.zip → estabelecimentos (rows=4753436)\n",
      "\n",
      "— Baixando: Estabelecimentos7.zip  → dataset='estabelecimentos'\n",
      "  ✅ ok: Estabelecimentos7.zip → estabelecimentos (rows=4753435)\n",
      "\n",
      "— Baixando: Estabelecimentos8.zip  → dataset='estabelecimentos'\n",
      "  ✅ ok: Estabelecimentos8.zip → estabelecimentos (rows=4753435)\n",
      "\n",
      "— Baixando: Estabelecimentos9.zip  → dataset='estabelecimentos'\n",
      "  ✅ ok: Estabelecimentos9.zip → estabelecimentos (rows=4753438)\n",
      "\n",
      "— Baixando: Motivos.zip  → dataset='desconhecido'\n",
      "  [WARN] Schema desconhecido — pulando este zip.\n",
      "\n",
      "— Baixando: Municipios.zip  → dataset='municipios'\n",
      "  ✅ ok: Municipios.zip → municipios (rows=5572)\n",
      "\n",
      "— Baixando: Naturezas.zip  → dataset='naturezas_juridicas'\n",
      "  ✅ ok: Naturezas.zip → naturezas_juridicas (rows=91)\n",
      "\n",
      "— Baixando: Paises.zip  → dataset='paises'\n",
      "  ✅ ok: Paises.zip → paises (rows=255)\n",
      "\n",
      "— Baixando: Qualificacoes.zip  → dataset='qualificacoes_socios'\n",
      "  ✅ ok: Qualificacoes.zip → qualificacoes_socios (rows=68)\n",
      "\n",
      "— Baixando: Simples.zip  → dataset='dados_do_simples'\n",
      "  ✅ ok: Simples.zip → dados_do_simples (rows=45035494)\n",
      "\n",
      "— Baixando: Socios0.zip  → dataset='socios'\n",
      "  ✅ ok: Socios0.zip → socios (rows=8199230)\n",
      "\n",
      "— Baixando: Socios1.zip  → dataset='socios'\n",
      "  ✅ ok: Socios1.zip → socios (rows=2019150)\n",
      "\n",
      "— Baixando: Socios2.zip  → dataset='socios'\n",
      "  ✅ ok: Socios2.zip → socios (rows=2019150)\n",
      "\n",
      "— Baixando: Socios3.zip  → dataset='socios'\n",
      "  ✅ ok: Socios3.zip → socios (rows=2019150)\n",
      "\n",
      "— Baixando: Socios4.zip  → dataset='socios'\n",
      "  ✅ ok: Socios4.zip → socios (rows=2019150)\n",
      "\n",
      "— Baixando: Socios5.zip  → dataset='socios'\n",
      "  ✅ ok: Socios5.zip → socios (rows=2019150)\n",
      "\n",
      "— Baixando: Socios6.zip  → dataset='socios'\n",
      "  ✅ ok: Socios6.zip → socios (rows=2019150)\n",
      "\n",
      "— Baixando: Socios7.zip  → dataset='socios'\n",
      "  ✅ ok: Socios7.zip → socios (rows=2019150)\n",
      "\n",
      "— Baixando: Socios8.zip  → dataset='socios'\n",
      "  ✅ ok: Socios8.zip → socios (rows=2019150)\n",
      "\n",
      "— Baixando: Socios9.zip  → dataset='socios'\n",
      "  ✅ ok: Socios9.zip → socios (rows=2019150)\n",
      "\n",
      "===== RESUMO =====\n",
      "✔️ processados: 35\n",
      "⏭️ pulados:     2\n",
      "❌ erros:        0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[file_url: string, zip_name: string, dataset: string, ref_month: string, etag: string, last_modified: string, content_length: string, content_md5: string, processed_at: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === PIPELINE DO MÊS MAIS RECENTE (varre todos .zip) ===\n",
    "import os, io, re, zipfile, shutil, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ByteType, DoubleType, TimestampType\n",
    "\n",
    "# -------- helpers locais da célula (auto-contidos) ----------\n",
    "def is_delta_enabled(spark) -> bool:\n",
    "    try:\n",
    "        ext = spark.conf.get(\"spark.sql.extensions\")\n",
    "        cat = spark.conf.get(\"spark.sql.catalog.spark_catalog\")\n",
    "        return (\"delta\" in (ext or \"\").lower()) and (\"delta\" in (cat or \"\").lower())\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def to_spark_type(t:str):\n",
    "    t = (t or \"\").lower()\n",
    "    if t in (\"string\",\"str\"): return StringType()\n",
    "    if t in (\"byte\",\"int8\"):  return ByteType()\n",
    "    if t in (\"double\",\"float64\",\"float\",\"decimal\"): return DoubleType()\n",
    "    return StringType()\n",
    "\n",
    "LOG_SCHEMA = (StructType()\n",
    "    .add(\"file_url\",\"string\")\n",
    "    .add(\"zip_name\",\"string\")\n",
    "    .add(\"dataset\",\"string\")\n",
    "    .add(\"ref_month\",\"string\")\n",
    "    .add(\"etag\",\"string\")\n",
    "    .add(\"last_modified\",\"string\")\n",
    "    .add(\"content_length\",\"string\")\n",
    "    .add(\"content_md5\",\"string\")\n",
    "    .add(\"processed_at\", TimestampType()))\n",
    "\n",
    "def read_log_df():\n",
    "    p = LOG_PATH\n",
    "    if p.exists():\n",
    "        return spark.read.parquet(str(p))\n",
    "    else:\n",
    "        return spark.createDataFrame([], LOG_SCHEMA)\n",
    "\n",
    "def write_log_append(df):\n",
    "    # append com 1 arquivo (log pequeno)\n",
    "    (df.coalesce(1)\n",
    "       .write.mode(\"append\")\n",
    "       .parquet(str(LOG_PATH)))\n",
    "\n",
    "def already_processed(logdf, file_url, ref_month, content_md5, etag, last_modified, content_length) -> bool:\n",
    "    cond = (\n",
    "        (logdf.file_url == file_url) &\n",
    "        (logdf.ref_month == ref_month) &\n",
    "        (\n",
    "            (logdf.content_md5 == content_md5) | \n",
    "            (\n",
    "                (logdf.etag == etag) &\n",
    "                (logdf.last_modified == last_modified) &\n",
    "                (logdf.content_length == content_length)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return logdf.filter(cond).limit(1).count() > 0\n",
    "\n",
    "def extract_zip_to_csvs(local_zip: Path, extract_dir: Path) -> list[Path]:\n",
    "    \"\"\"Extrai tudo e força extensão .csv nos arquivos de dados.\"\"\"\n",
    "    if extract_dir.exists():\n",
    "        shutil.rmtree(extract_dir)\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    csvs = []\n",
    "    with zipfile.ZipFile(local_zip, \"r\") as z:\n",
    "        for m in z.infolist():\n",
    "            if m.is_dir():\n",
    "                continue\n",
    "            out_name = Path(m.filename).name\n",
    "            if not out_name.lower().endswith(\".csv\"):\n",
    "                out_name = out_name + \".csv\"\n",
    "            z.extract(m, extract_dir)\n",
    "            src = extract_dir / m.filename\n",
    "            dst = extract_dir / out_name\n",
    "            if src != dst:\n",
    "                dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                src.replace(dst)\n",
    "            if dst.suffix.lower() == \".csv\":\n",
    "                csvs.append(dst)\n",
    "    return sorted(csvs)\n",
    "\n",
    "# -------- começa o fluxo ----------\n",
    "months = month_dirs()\n",
    "if not months:\n",
    "    raise SystemExit(\"Nenhum mês encontrado na raiz do site.\")\n",
    "\n",
    "target_month = months[-1]              # ex.: '2025-09/'\n",
    "ref_month = target_month.strip(\"/\")    # '2025-09'\n",
    "print(f\"➡️  Mês alvo: {ref_month}\")\n",
    "\n",
    "zips = list_zip_urls(target_month)\n",
    "print(f\"🧾 {len(zips)} arquivos .zip encontrados no mês {ref_month}.\")\n",
    "\n",
    "USE_DELTA = False\n",
    "print(f\"💾 Formato de saída: {'Delta Lake' if USE_DELTA else 'Parquet'}\")\n",
    "\n",
    "logdf = read_log_df()\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "errors = []\n",
    "\n",
    "for zip_name, zip_url in zips:\n",
    "    ds_name, schema_cols = pick_schema_for_zip(zip_name)\n",
    "    print(f\"\\n— Baixando: {zip_name}  → dataset='{ds_name}'\")\n",
    "\n",
    "    local_zip = RAW_DIR / f\"{ref_month}_{zip_name}\"\n",
    "    try:\n",
    "        meta = download(zip_url, local_zip)\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERRO] Falha no download: {e}\")\n",
    "        errors.append((zip_name, \"download\", str(e)))\n",
    "        continue\n",
    "\n",
    "    # verificação de já processado\n",
    "    try:\n",
    "        if already_processed(logdf, zip_url, ref_month, meta[\"content_md5\"], meta[\"etag\"], meta[\"last_modified\"], meta[\"content_length\"]):\n",
    "            print(\"  ✓ já processado anteriormente (mesmos metadados) — pulando.\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] Não consegui consultar log (seguindo em frente): {e}\")\n",
    "\n",
    "    # schema\n",
    "    if not schema_cols:\n",
    "        print(\"  [WARN] Schema desconhecido — pulando este zip.\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    # extrai e lê CSV(s)\n",
    "    try:\n",
    "        tmp_dir = RAW_DIR / f\"_extract_{ref_month}_{Path(zip_name).stem}\"\n",
    "        csv_paths = extract_zip_to_csvs(local_zip, tmp_dir)\n",
    "        if not csv_paths:\n",
    "            print(\"  [WARN] ZIP sem CSVs legíveis — pulando.\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        # monta schema Spark\n",
    "        schema = StructType([StructField(col, to_spark_type(tp), True) for col, tp in schema_cols])\n",
    "\n",
    "        # concatena todos os CSVs do zip (se houver mais de um)\n",
    "        df_parts = []\n",
    "        for csv_path in csv_paths:\n",
    "            df_part = (spark.read\n",
    "                       .format(\"csv\")\n",
    "                       .option(\"header\",\"false\")\n",
    "                       .option(\"sep\",\";\")\n",
    "                       .option(\"encoding\",\"ISO-8859-1\")   # <- importante no Spark\n",
    "                       .schema(schema)\n",
    "                       .load(str(csv_path)))\n",
    "            df_parts.append(df_part)\n",
    "\n",
    "        from functools import reduce\n",
    "        from pyspark.sql import DataFrame as SDF\n",
    "        df = reduce(SDF.unionByName, df_parts) if len(df_parts) > 1 else df_parts[0]\n",
    "\n",
    "        # adiciona ref_month e grava\n",
    "        df2 = df.withColumn(\"ref_month\", F.lit(ref_month)).coalesce(1)\n",
    "        dest = OUT_DIR / ds_name\n",
    "\n",
    "        if USE_DELTA:\n",
    "            (df2.write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"overwrite\")                  # overwrite dinâmico só nas partições tocadas\n",
    "                .partitionBy(\"ref_month\")\n",
    "                .save(str(dest)))\n",
    "        else:\n",
    "            (df2.write\n",
    "                .mode(\"overwrite\")\n",
    "                .partitionBy(\"ref_month\")\n",
    "                .parquet(str(dest)))\n",
    "\n",
    "        # atualiza log\n",
    "        now = datetime.utcnow()\n",
    "        rows = [(zip_url, zip_name, ds_name, ref_month,\n",
    "                 meta[\"etag\"], meta[\"last_modified\"], meta[\"content_length\"], meta[\"content_md5\"], now)]\n",
    "        to_append = spark.createDataFrame(rows, schema=LOG_SCHEMA)\n",
    "        write_log_append(to_append)\n",
    "\n",
    "        print(f\"  ✅ ok: {zip_name} → {ds_name} (rows={df2.count()})\")\n",
    "        processed_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERRO] Processando {zip_name}: {e}\")\n",
    "        errors.append((zip_name, \"process\", str(e)))\n",
    "        continue\n",
    "\n",
    "print(\"\\n===== RESUMO =====\")\n",
    "print(f\"✔️ processados: {processed_count}\")\n",
    "print(f\"⏭️ pulados:     {skipped_count}\")\n",
    "print(f\"❌ erros:        {len(errors)}\")\n",
    "if errors:\n",
    "    for name, where, msg in errors[:10]:\n",
    "        print(f\"  - {name} @ {where}: {msg}\")\n",
    "\n",
    "# espiada no log final\n",
    "final_log = read_log_df()\n",
    "display(final_log.orderBy(F.col(\"processed_at\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67617d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
