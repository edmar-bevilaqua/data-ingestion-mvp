{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00672616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "Pipeline local dos Dados Abertos CNPJ (Receita Federal)\n",
    "Windows + Python 3.11\n",
    "\n",
    "Uso:\n",
    "  python cnae.py           # processa somente o mês mais recente\n",
    "  python cnae.py --all     # (opcional) carga histórica: todos os meses\n",
    "\n",
    "Saída (padrão):\n",
    "  .\\data\\parquet\\<dataset>\\{YYYY-MM}\\part-*.parquet\n",
    "  .\\data\\_metadata\\ingestion_log.parquet\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import zipfile\n",
    "import shutil\n",
    "import tempfile\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import unicodedata, difflib\n",
    "\n",
    "# schemas conhecidos (colunas e tipos)\n",
    "# DOC: https://www.gov.br/receitafederal/dados/cnpj-metadados.pdf\n",
    "schemas = {\n",
    "    \"empresas\": {\n",
    "        \"columns\": {\n",
    "            \"cnpj_basico\": \"string\",\n",
    "            \"razao_social\": \"string\",\n",
    "            \"natureza_juridica\": \"string\",\n",
    "            \"qualificacao_responsavel\": \"string\",\n",
    "            \"capital_social\": \"float64\",\n",
    "            \"porte\": \"string\",\n",
    "            \"ente_federativo_responsavel\": \"string\",\n",
    "        }\n",
    "    },\n",
    "    \"estabelecimentos\": {\n",
    "        \"columns\": {\n",
    "            \"cnpj_basico\": \"string\",\n",
    "            \"cnpj_ordem\": \"string\",\n",
    "            \"cnpj_dv\": \"string\",\n",
    "            \"identificador_matriz_filial\": \"int8\",\n",
    "            \"nome_fantasia\": \"string\",\n",
    "            \"situacao_cadastral\": \"string\",\n",
    "            \"data_situacao_cadastral\": \"string\",\n",
    "            \"motivo_situacao_cadastral\": \"string\",\n",
    "            \"nome_cidade_exterior\": \"string\",\n",
    "            \"pais\": \"string\",\n",
    "            \"data_inicio_atividade\": \"string\",\n",
    "            \"cnae_fiscal_principal\": \"string\",\n",
    "            \"cnae_fiscal_secundaria\": \"string\",\n",
    "            \"tipo_logradouro\": \"string\",\n",
    "            \"logradouro\": \"string\",\n",
    "            \"numero\": \"string\",\n",
    "            \"complemento\": \"string\",\n",
    "            \"bairro\": \"string\",\n",
    "            \"cep\": \"string\",\n",
    "            \"uf\": \"string\",\n",
    "            \"municipio\": \"string\",\n",
    "            \"ddd_telefone_1\": \"string\",\n",
    "            \"telefone_1\": \"string\",\n",
    "            \"ddd_telefone_2\": \"string\",\n",
    "            \"telefone_2\": \"string\",\n",
    "            \"ddd_fax\": \"string\",\n",
    "            \"fax\": \"string\",\n",
    "            \"email\": \"string\",\n",
    "            \"situacao_especial\": \"string\",\n",
    "            \"data_situacao_especial\": \"string\",\n",
    "        }\n",
    "    },\n",
    "    \"dados_do_simples\": {\n",
    "        \"columns\": {\n",
    "            \"cnpj_basico\": \"string\",\n",
    "            \"opcao_pelo_simples\": \"string\",\n",
    "            \"data_opcao_pelo_simples\": \"string\",\n",
    "            \"data_exclusao_do_simples\": \"string\",\n",
    "            \"opcao_pelo_mei\": \"string\",\n",
    "            \"data_opcao_pelo_mei\": \"string\",\n",
    "            \"data_exclusao_do_mei\": \"string\",\n",
    "        }\n",
    "    },\n",
    "    \"socios\": {\n",
    "        \"columns\": {\n",
    "            \"cnpj_basico\": \"string\",\n",
    "            \"identificador_socio\": \"string\",\n",
    "            \"nome_socio\": \"string\",\n",
    "            \"cnpj_cpf_socio\": \"string\",\n",
    "            \"qualificacao_socio\": \"string\",\n",
    "            \"data_entrada_sociedade\": \"string\",\n",
    "            \"pais\": \"string\",\n",
    "            \"representante_legal\": \"string\",\n",
    "            \"nome_representante_legal\": \"string\",\n",
    "            \"qualificacao_representante_legal\": \"string\",\n",
    "            \"faixa_etaria\": \"string\",\n",
    "        }\n",
    "    },\n",
    "    \"paises\": {\n",
    "        \"columns\": {\n",
    "            \"codigo\": \"string\",\n",
    "            \"descricao\": \"string\",\n",
    "        }\n",
    "    },\n",
    "    \"municipios\": {\n",
    "        \"columns\": {\n",
    "            \"codigo\": \"string\",\n",
    "            \"descricao\": \"string\",\n",
    "        }\n",
    "    },\n",
    "    \"qualificacoes_socios\": {\n",
    "        \"columns\": {\n",
    "            \"codigo\": \"string\",\n",
    "            \"descricao\": \"string\",\n",
    "        }\n",
    "    },\n",
    "    \"naturezas_juridicas\": {\n",
    "        \"columns\": {\n",
    "            \"codigo\": \"string\",\n",
    "            \"descricao\": \"string\",\n",
    "        }\n",
    "    },\n",
    "    \"cnaes\": {\n",
    "        \"columns\": {\n",
    "            \"codigo\": \"string\",\n",
    "            \"descricao\": \"string\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# mapeamentos de sinônimos/variações comuns\n",
    "SCHEMA_SYNONYMS = {\n",
    "    \"simples\": \"dados_do_simples\",\n",
    "    \"dados_simples\": \"dados_do_simples\",\n",
    "    \"qualificacoes\": \"qualificacoes_socios\",\n",
    "    \"qualificacao_socio\": \"qualificacoes_socios\",\n",
    "    \"natureza_juridica\": \"naturezas_juridicas\",\n",
    "    \"municipio\": \"municipios\",\n",
    "    \"pais\": \"paises\",\n",
    "    \"cnae\": \"cnaes\",\n",
    "    \"empresa\": \"empresas\",\n",
    "    \"estabelecimento\": \"estabelecimentos\",\n",
    "    \"socio\": \"socios\",\n",
    "}\n",
    "\n",
    "# pistas por padrões de nome de arquivo (útil para arquivos tipo F.K03200$Z.D50913.CNAECSV)\n",
    "FILENAME_HINTS = [\n",
    "    (r\"cnae\", \"cnaes\"),\n",
    "    (r\"simple|mei\", \"dados_do_simples\"),\n",
    "    (r\"soci\", \"socios\"),\n",
    "    (r\"estab\", \"estabelecimentos\"),\n",
    "    (r\"empre\", \"empresas\"),\n",
    "    (r\"qualif\", \"qualificacoes_socios\"),\n",
    "    (r\"nat.?jur|natureza\", \"naturezas_juridicas\"),\n",
    "    (r\"munic\", \"municipios\"),\n",
    "    (r\"pais|pa[ií]s\", \"paises\"),\n",
    "]\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = s.strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def _apply_synonyms(key: str) -> str:\n",
    "    key = _slug(key)\n",
    "    return SCHEMA_SYNONYMS.get(key, key)\n",
    "\n",
    "def choose_schema_key(preferred_hint: str | None, filename_hint: str | None) -> str | None:\n",
    "    \"\"\"\n",
    "    Decide qual schema usar:\n",
    "      1) tenta sinônimos e match exato pelo 'preferred_hint' (ex.: dataset inferido)\n",
    "      2) tenta pistas do filename (regex)\n",
    "      3) fuzzy match contra as chaves de 'schemas'\n",
    "    \"\"\"\n",
    "    keys = list(schemas.keys())\n",
    "    norm_keys = {_slug(k): k for k in keys}\n",
    "\n",
    "    # 1) pelo preferred_hint (dataset inferido)\n",
    "    if preferred_hint:\n",
    "        k1 = _apply_synonyms(preferred_hint)\n",
    "        if k1 in norm_keys:\n",
    "            return norm_keys[k1]\n",
    "\n",
    "    # 2) por pistas do filename\n",
    "    if filename_hint:\n",
    "        fname = _slug(filename_hint)\n",
    "        for pat, target in FILENAME_HINTS:\n",
    "            if re.search(pat, fname):\n",
    "                t = _apply_synonyms(target)\n",
    "                if t in norm_keys:\n",
    "                    return norm_keys[t]\n",
    "\n",
    "    # 3) fuzzy contra as chaves\n",
    "    candidates = list(norm_keys.keys())\n",
    "    query = _slug(preferred_hint or filename_hint or \"\")\n",
    "    matches = difflib.get_close_matches(query, candidates, n=1, cutoff=0.65)\n",
    "    if matches:\n",
    "        return norm_keys[matches[0]]\n",
    "\n",
    "    return None\n",
    "\n",
    "def schema_columns_and_types(schema_key: str) -> tuple[list[str], dict]:\n",
    "    cols = schemas[schema_key][\"columns\"]\n",
    "    colnames = list(cols.keys())  # ordem fornecida no dicionário\n",
    "    types = cols  # mapping name -> dtype string\n",
    "    return colnames, types\n",
    "\n",
    "def cast_df_by_types(df: pd.DataFrame, types: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converte colunas conforme o dict 'types' com segurança.\n",
    "    Estratégia: lemos TUDO como string e depois:\n",
    "      - float64: to_numeric(errors='coerce')\n",
    "      - int8/int16/int32/int64: to_numeric + cast para pandas nullable (Int8/Int32...)\n",
    "      - string: garantir dtype 'string'\n",
    "    \"\"\"\n",
    "    for col, dt in types.items():\n",
    "        if col not in df.columns:\n",
    "            # se a linha tinha menos colunas, garante coluna vazia\n",
    "            df[col] = pd.NA\n",
    "\n",
    "        if dt == \"string\":\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "        elif dt.startswith(\"float\"):\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float64\")\n",
    "        elif dt.startswith(\"int\"):\n",
    "            # escolhe dtype pandas nullable equivalente\n",
    "            width = re.findall(r\"\\d+\", dt)\n",
    "            width = int(width[0]) if width else 64\n",
    "            nullable = {8:\"Int8\",16:\"Int16\",32:\"Int32\",64:\"Int64\"}.get(width, \"Int64\")\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(nullable)\n",
    "        else:\n",
    "            # fallback: string\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_URL = \"https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/\"\n",
    "\n",
    "# pastas locais (podem ser relativas)\n",
    "DATA_ROOT = Path(\"./data\").resolve()\n",
    "PARQUET_ROOT = DATA_ROOT / \"parquet\"\n",
    "META_ROOT = DATA_ROOT / \"_metadata\"\n",
    "INGEST_LOG_PATH = META_ROOT / \"ingestion_log.parquet\"\n",
    "\n",
    "# performance / robustez\n",
    "CHUNK_ROWS = 1_000_000           # ajuste conforme sua RAM\n",
    "TIMEOUT = (20, 120)            # (connect, read) em segundos\n",
    "USER_AGENT = \"CNPJ-Pipeline-Local/1.0 (+Windows Python 3.11)\"\n",
    "ENCODINGS_TRY = (\"utf-8\", \"latin-1\")  # tentativa de leitura\n",
    "\n",
    "# mapeia nomes de arquivos para datasets “canônicos”\n",
    "DATASET_ALIASES = {\n",
    "    \"empresas\": \"empresas\",\n",
    "    \"estabelecimentos\": \"estabelecimentos\",\n",
    "    \"socios\": \"socios\",\n",
    "    \"simples\": \"simples\",\n",
    "    \"cnaes\": \"cnaes\",\n",
    "    \"municipios\": \"municipios\",\n",
    "    \"pais\": \"paises\",\n",
    "    \"paises\": \"paises\",\n",
    "    \"naturezas_juridicas\": \"naturezas_juridicas\",\n",
    "    \"qualificacoes\": \"qualificacoes\",\n",
    "    \"motivos\": \"motivos\",\n",
    "    \"membros\": \"membros\",\n",
    "}\n",
    "\n",
    "# cria pastas\n",
    "PARQUET_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "META_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# HTTP com retry\n",
    "# =========================\n",
    "def http_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.8,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=frozenset([\"GET\", \"HEAD\"]),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update({\"User-Agent\": USER_AGENT})\n",
    "    return s\n",
    "\n",
    "SESSION = http_session()\n",
    "\n",
    "# =========================\n",
    "# Utilitários\n",
    "# =========================\n",
    "def list_month_dirs() -> List[str]:\n",
    "    \"\"\"\n",
    "    Varre o índice HTML e retorna nomes tipo 'YYYY-MM/'.\n",
    "    \"\"\"\n",
    "    r = SESSION.get(BASE_URL, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    html = r.text\n",
    "\n",
    "    months = sorted(set(re.findall(r'href=\"(\\d{4}-\\d{2}/)\"', html)))\n",
    "    if not months:\n",
    "        # fallback\n",
    "        months = sorted(set(re.findall(r\">\\s*(\\d{4}-\\d{2})/\\s*<\", html)))\n",
    "        months = [m + \"/\" for m in months]\n",
    "\n",
    "    if not months:\n",
    "        raise RuntimeError(\"Não consegui localizar diretórios mensais no índice.\")\n",
    "    return months\n",
    "\n",
    "def latest_month_dir() -> str:\n",
    "    months = list_month_dirs()\n",
    "    months_sorted = sorted(months, key=lambda m: m.rstrip(\"/\"))\n",
    "    return months_sorted[-1]\n",
    "\n",
    "def list_zip_urls(month_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Lista URLs de .zip dentro do diretório mensal.\n",
    "    \"\"\"\n",
    "    url = urljoin(BASE_URL, month_dir)\n",
    "    r = SESSION.get(url, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    html = r.text\n",
    "    files = re.findall(r'href=\"([^\"]+\\.zip)\"', html, flags=re.IGNORECASE)\n",
    "    files = sorted(set(files))\n",
    "    return [urljoin(url, f) for f in files]\n",
    "\n",
    "def head_info(url: str) -> Dict[str, Optional[str]]:\n",
    "    try:\n",
    "        h = SESSION.head(url, timeout=TIMEOUT, allow_redirects=True)\n",
    "        if h.status_code >= 400:\n",
    "            h = SESSION.get(url, stream=True, timeout=TIMEOUT)\n",
    "        headers = h.headers\n",
    "        return {\n",
    "            \"etag\": headers.get(\"ETag\"),\n",
    "            \"last_modified\": headers.get(\"Last-Modified\"),\n",
    "            \"content_length\": headers.get(\"Content-Length\"),\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"etag\": None, \"last_modified\": None, \"content_length\": None}\n",
    "\n",
    "def md5_bytes(b: bytes) -> str:\n",
    "    m = hashlib.md5()\n",
    "    m.update(b)\n",
    "    return m.hexdigest()\n",
    "\n",
    "def infer_dataset_name(zip_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrai um nome de dataset amigável com base no nome do zip.\n",
    "    \"\"\"\n",
    "    base = os.path.basename(zip_name).lower()\n",
    "    base = re.sub(r\"\\.zip$\", \"\", base)\n",
    "    for k, v in DATASET_ALIASES.items():\n",
    "        if k in base:\n",
    "            return v\n",
    "    base = re.sub(r\"[^a-z0-9]+\", \"_\", base).strip(\"_\")\n",
    "    return base or \"dataset\"\n",
    "\n",
    "def ensure_ingestion_log() -> pd.DataFrame:\n",
    "    if INGEST_LOG_PATH.exists():\n",
    "        return pd.read_parquet(INGEST_LOG_PATH)\n",
    "    cols = [\n",
    "        \"file_url\",\n",
    "        \"zip_name\",\n",
    "        \"dataset\",\n",
    "        \"ref_month\",\n",
    "        \"etag\",\n",
    "        \"last_modified\",\n",
    "        \"content_length\",\n",
    "        \"content_md5\",\n",
    "        \"processed_at\",\n",
    "    ]\n",
    "    return pd.DataFrame(columns=cols)\n",
    "\n",
    "def save_ingestion_log(df: pd.DataFrame) -> None:\n",
    "    df.to_parquet(INGEST_LOG_PATH, index=False)\n",
    "\n",
    "def already_processed(\n",
    "    logdf: pd.DataFrame,\n",
    "    file_url: str,\n",
    "    ref_month: str,\n",
    "    md5: Optional[str],\n",
    "    etag: Optional[str],\n",
    "    last_modified: Optional[str],\n",
    "    content_length: Optional[str],\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Verifica se já processamos esse conteúdo exato (preferindo MD5).\n",
    "    \"\"\"\n",
    "    if logdf.empty:\n",
    "        return False\n",
    "    subset = logdf[(logdf[\"file_url\"] == file_url) & (logdf[\"ref_month\"] == ref_month)]\n",
    "    if subset.empty:\n",
    "        return False\n",
    "    if md5:\n",
    "        return (subset[\"content_md5\"] == md5).any()\n",
    "    mask = True\n",
    "    if etag:\n",
    "        mask &= subset[\"etag\"] == etag\n",
    "    if last_modified:\n",
    "        mask &= subset[\"last_modified\"] == last_modified\n",
    "    if content_length:\n",
    "        mask &= subset[\"content_length\"] == content_length\n",
    "    return bool(subset[mask].shape[0] > 0)\n",
    "\n",
    "def write_chunk_parquet(df_chunk: pd.DataFrame, dataset: str, ref_month: str) -> None:\n",
    "    out_dir = PARQUET_ROOT / dataset / f\"{ref_month}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fname = f\"part-{int(time.time()*1e6)}-{hashlib.md5(str(df_chunk.shape).encode()).hexdigest()[:8]}.parquet\"\n",
    "    df_chunk.to_parquet(out_dir / fname, index=False)\n",
    "\n",
    "def process_zip_to_parquet(zip_bytes: bytes, dataset: str, ref_month: str) -> None:\n",
    "    \"\"\"\n",
    "    - Extrai todos os membros do ZIP.\n",
    "    - Se o nome terminar sem extensão mas sugerir CSV (ex.: ...CNAECSV), renomeia para .csv.\n",
    "    - Escolhe o schema correto por aproximação (dataset + nome do arquivo).\n",
    "    - Lê SEM cabeçalho (header=None) e aplica os nomes na **ordem** do schema.\n",
    "    - Faz cast dos tipos conforme o schema.\n",
    "    - Grava em Parquet particionado.\n",
    "    \"\"\"\n",
    "    tmp_dir = Path(tempfile.mkdtemp(prefix=\"cnpj_zip_\"))\n",
    "    try:\n",
    "        zpath = tmp_dir / \"file.zip\"\n",
    "        zpath.write_bytes(zip_bytes)\n",
    "\n",
    "        with zipfile.ZipFile(zpath) as z:\n",
    "            for info in z.infolist():\n",
    "                if info.is_dir():\n",
    "                    continue\n",
    "\n",
    "                extracted_path = Path(z.extract(info, tmp_dir))\n",
    "                original_name = extracted_path.name\n",
    "                name_upper = original_name.upper()\n",
    "                name_lower = original_name.lower()\n",
    "\n",
    "                # --- normalização: garantir .csv no fim quando \"parece\" CSV ---\n",
    "                has_good_ext = bool(re.search(r\"\\.(csv|txt)$\", name_lower))\n",
    "                looks_like_csv = (\n",
    "                    name_upper.endswith(\"CSV\")\n",
    "                    or \"CNAECSV\" in name_upper\n",
    "                    or re.search(r\"(?:^|[\\W_])CSV(?:[\\W_]|$)\", name_upper) is not None\n",
    "                )\n",
    "                normalized_path = extracted_path\n",
    "                if not has_good_ext and looks_like_csv:\n",
    "                    normalized_path = extracted_path.with_name(original_name + \".csv\")\n",
    "                    try:\n",
    "                        extracted_path.rename(normalized_path)\n",
    "                    except FileExistsError:\n",
    "                        normalized_path = extracted_path.with_name(original_name + f\".{int(time.time())}.csv\")\n",
    "                        extracted_path.rename(normalized_path)\n",
    "\n",
    "                # --- escolher schema por aproximação ---\n",
    "                schema_key = choose_schema_key(preferred_hint=dataset, filename_hint=original_name)\n",
    "                if not schema_key:\n",
    "                    print(f\"[WARN] Sem schema para '{original_name}' (dataset hint='{dataset}'). Ignorando.\")\n",
    "                    continue\n",
    "\n",
    "                colnames, types = schema_columns_and_types(schema_key)\n",
    "                ncols = len(colnames)\n",
    "\n",
    "                # --- leitura em chunks SEM cabeçalho ---\n",
    "                read_kwargs_base = dict(\n",
    "                    sep=\";\",\n",
    "                    header=None,          # não há cabeçalho!\n",
    "                    names=colnames,       # aplica ordem do schema\n",
    "                    usecols=range(ncols), # ignora colunas extras\n",
    "                    dtype=str,            # lê tudo como string; depois fazemos cast\n",
    "                    on_bad_lines=\"skip\",\n",
    "                )\n",
    "\n",
    "                read_ok = False\n",
    "                for enc in ENCODINGS_TRY:\n",
    "                    # tentamos primeiro com engine C (rápido e aceita low_memory),\n",
    "                    # depois fallback para engine python (sem low_memory).\n",
    "                    for eng in (\"c\", \"python\"):\n",
    "                        try:\n",
    "                            read_kwargs = read_kwargs_base.copy()\n",
    "                            read_kwargs[\"encoding\"] = enc\n",
    "                            read_kwargs[\"engine\"] = eng\n",
    "                            if eng == \"c\":\n",
    "                                read_kwargs[\"low_memory\"] = False  # suportado só no engine C\n",
    "                            # (opcional) tornar parsing mais robusto:\n",
    "                            # read_kwargs[\"quoting\"] = _csv.QUOTE_MINIMAL\n",
    "\n",
    "                            for chunk in pd.read_csv(normalized_path, chunksize=CHUNK_ROWS, **read_kwargs):\n",
    "                                # garante todas as colunas esperadas\n",
    "                                for c in colnames:\n",
    "                                    if c not in chunk.columns:\n",
    "                                        chunk[c] = pd.NA\n",
    "                                # cast seguro\n",
    "                                chunk = cast_df_by_types(chunk, types)\n",
    "                                # colunas auxiliares\n",
    "                                chunk[\"ref_month\"] = ref_month\n",
    "                                chunk[\"ingestion_ts\"] = pd.Timestamp(datetime.now(timezone.utc))\n",
    "                                write_chunk_parquet(chunk, schema_key, ref_month)\n",
    "\n",
    "                            read_ok = True\n",
    "                            break  # saiu do loop de engines\n",
    "                        except UnicodeDecodeError:\n",
    "                            # tenta o próximo encoding\n",
    "                            break  # muda o enc rapidamente\n",
    "                        except pd.errors.ParserError as e:\n",
    "                            # se o engine C reclamar, tentaremos o python\n",
    "                            if eng == \"c\":\n",
    "                                continue\n",
    "                            print(f\"[WARN] ParserError em '{original_name}' com engine={eng}: {e}\")\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            # outros erros: tenta engine alternativo ou próximo encoding\n",
    "                            if eng == \"c\":\n",
    "                                continue\n",
    "                            print(f\"[WARN] Falha lendo '{original_name}' enc='{enc}' engine='{eng}': {e}\")\n",
    "                            break\n",
    "                    if read_ok:\n",
    "                        break\n",
    "\n",
    "                if not read_ok:\n",
    "                    print(f\"[INFO] Ignorando '{original_name}' — não consegui ler como CSV com o schema '{schema_key}'.\")\n",
    "\n",
    "    finally:\n",
    "        shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "\n",
    "def run_pipeline(process_only_latest: bool = True) -> None:\n",
    "    logdf = ensure_ingestion_log()\n",
    "\n",
    "    months = list_month_dirs()\n",
    "    if process_only_latest:\n",
    "        months = [sorted(months)[-1]]\n",
    "\n",
    "    print(f\"Meses alvo: {months}\")\n",
    "    for month_dir in months:\n",
    "        ref_month = month_dir.rstrip(\"/\")\n",
    "        zip_urls = list_zip_urls(month_dir)\n",
    "        if not zip_urls:\n",
    "            print(f\"[{ref_month}] Nenhum .zip encontrado.\")\n",
    "            continue\n",
    "        zip_urls = zip_urls[0:3]  # DEBUG: limitar para testes\n",
    "\n",
    "        print(f\"[{ref_month}] {len(zip_urls)} arquivos .zip encontrados.\")\n",
    "        for url in zip_urls:\n",
    "            headers = head_info(url)\n",
    "            etag = headers.get(\"etag\")\n",
    "            last_modified = headers.get(\"last_modified\")\n",
    "            content_length = headers.get(\"content_length\")\n",
    "\n",
    "            zip_name = os.path.basename(urlparse(url).path)\n",
    "            dataset = infer_dataset_name(zip_name)\n",
    "\n",
    "            print(f\"  - baixando {zip_name} ...\")\n",
    "            resp = SESSION.get(url, timeout=TIMEOUT)\n",
    "            resp.raise_for_status()\n",
    "            b = resp.content\n",
    "            content_md5 = md5_bytes(b)\n",
    "\n",
    "            if already_processed(logdf, url, ref_month, content_md5, etag, last_modified, content_length):\n",
    "                print(f\"    > sem mudanças — pulando {zip_name}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"    > processando dataset='{dataset}' ...\")\n",
    "            process_zip_to_parquet(b, dataset, ref_month)\n",
    "\n",
    "            # atualiza log\n",
    "            newrow = {\n",
    "                \"file_url\": url,\n",
    "                \"zip_name\": zip_name,\n",
    "                \"dataset\": dataset,\n",
    "                \"ref_month\": ref_month,\n",
    "                \"etag\": etag,\n",
    "                \"last_modified\": last_modified,\n",
    "                \"content_length\": content_length,\n",
    "                \"content_md5\": content_md5,\n",
    "                \"processed_at\": pd.Timestamp(datetime.now(timezone.utc)),\n",
    "            }\n",
    "            logdf = pd.concat([logdf, pd.DataFrame([newrow])], ignore_index=True)\n",
    "            save_ingestion_log(logdf)\n",
    "            print(f\"    > ok: {zip_name} (dataset={dataset})\")\n",
    "\n",
    "    print(\"Pipeline concluído.\")\n",
    "\n",
    "# =========================\n",
    "# CLI/Launcher compatível com Jupyter e terminal\n",
    "# =========================\n",
    "def main(argv=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Pipeline local CNPJ (dados_abertos_cnpj)\")\n",
    "    parser.add_argument(\"--all\", action=\"store_true\", help=\"Processa todos os meses (carga histórica)\")\n",
    "    # Ignora argumentos desconhecidos (ex.: --f=... do Jupyter)\n",
    "    args, _unknown = parser.parse_known_args(argv)\n",
    "    run_pipeline(process_only_latest=not args.all)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "    except Exception:\n",
    "        print(\"Aviso: 'pyarrow' não encontrado. Instale com: pip install pyarrow\", file=sys.stderr)\n",
    "\n",
    "    # Se estiver dentro de um kernel Jupyter, NÃO use os argv do kernel\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        # 1) rode padrão (mês mais recente) OU\n",
    "        # 2) se quiser a carga histórica dentro do notebook, mude para False\n",
    "        run_pipeline(process_only_latest=True)\n",
    "    else:\n",
    "        # Terminal/PowerShell: use os argumentos passados\n",
    "        main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bd8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(r\"C:\\Users\\edmar\\MeusProjetos\\white-cube\\data-ingestion-mvp\\data\\parquet\\cnaes\\2025-09\\part-1758205684616270-9d983dbc.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc74171",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(r\"C:\\Users\\edmar\\MeusProjetos\\white-cube\\data-ingestion-mvp\\data\\_metadata\\ingestion_log.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89355489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "white-cube-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
